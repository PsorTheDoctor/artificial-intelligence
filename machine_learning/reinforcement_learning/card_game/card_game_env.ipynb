{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/PsorTheDoctor/Sekcja-SI/blob/master/machine_learning/reinforcement_learning/OpenAI_gym/card_game_env.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "En02FxRwzPCD"
   },
   "source": [
    "# Card Game Env: Własne środowisko Pythona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hjV1uDG7q9_C",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35.0
    },
    "outputId": "cb653b22-0ac7-4151-d196-6f8335f6897c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 2.x selected.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  %tensorflow_version 2.x\n",
    "except:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ScyzGCBqq-zF"
   },
   "outputs": [],
   "source": [
    "!pip install tf-agents\n",
    "!pip install 'gym==0.10.11'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d2_UaUwrqnev"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import abc\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGE3CN6FbTwB"
   },
   "source": [
    "### Tworzenie własnego środowiska Pythona\n",
    "Załóżmy, że chcemy przetrenować agenta do poniżeszej gry karcianej (inspirowanej Black Jackiem):\n",
    "\n",
    "1.   W grze używa się skończonej talii kart ponumerowanych 1...10.\n",
    "2.   W każdej kolejce agent musi zrobić jedną z 2 rzeczy: wziąć losową kartę albo opuścić kolejkę.\n",
    "3.   Celem gry jest uzyskanie sumy kart tak bardzo jak top możliwe zbliżonej do 21 na końcu każdej rundy, bez przekraczania.\n",
    "\n",
    "Środowisko przedstawiające grę mogłoby wyglądać tak:\n",
    "\n",
    "1.   Akcje: Mamy 2 akcje. Akcja 0: weź nową kartę i Akcja 1: opuść kolejkę.\n",
    "2.   Obserwacje: Suma kart w bieżącej rundzie.\n",
    "3.   Nagroda: `sum_of_cards - 21 if sum_of_cards <= 21, else -21`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B7xMqltja9Q-"
   },
   "outputs": [],
   "source": [
    "class CardGameEnv(py_environment.PyEnvironment):\n",
    "\n",
    "  def __init__(self):\n",
    "    self._action_spec = array_spec.BoundedArraySpec(\n",
    "        shape=(), dtype=np.int32, minimum=0, maximum=1, name='action')\n",
    "    self._observation_spec = array_spec.BoundedArraySpec(\n",
    "        shape=(1,), dtype=np.int32, minimum=0, name='observation')\n",
    "    self._state = 0\n",
    "    self._episode_ended = False\n",
    "\n",
    "  def action_spec(self):\n",
    "    return self._action_spec\n",
    "\n",
    "  def observation_spec(self):\n",
    "    return self._observation_spec\n",
    "\n",
    "  def _reset(self):\n",
    "    self._state = 0\n",
    "    self._episode_ended = False\n",
    "    return ts.restart(np.array([self._state], dtype=np.int32))\n",
    "\n",
    "  def _step(self, action):\n",
    "\n",
    "    if self._episode_ended:\n",
    "      return self.reset()\n",
    "\n",
    "    if action == 1:\n",
    "      self._episode_ended = True\n",
    "    elif action == 0:\n",
    "      new_card = np.random.randint(1, 11)\n",
    "      self._state += new_card\n",
    "    else:\n",
    "      raise ValueError('`akcja` powinna być 0 lub 1.')\n",
    "\n",
    "    if self._episode_ended or self._state >= 21:\n",
    "      reward = self._state - 21 if self._state <= 21 else -21\n",
    "      return ts.termination(np.array([self._state], dtype=np.int32), reward)\n",
    "    else:\n",
    "      return ts.transition(\n",
    "          np.array([self._state], dtype=np.int32), reward=0.0, discount=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i6dCEYl_r1Oa"
   },
   "outputs": [],
   "source": [
    "environment = CardGameEnv()\n",
    "utils.validate_py_environment(environment, episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SVeaTtCvuhqc",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107.0
    },
    "outputId": "5e61a3e4-4613-438f-ef57-88a0ce049477"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(step_type=array(0, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([0], dtype=int32))\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([4], dtype=int32))\n",
      "TimeStep(step_type=array(2, dtype=int32), reward=array(-17., dtype=float32), discount=array(0., dtype=float32), observation=array([4], dtype=int32))\n",
      "Final Reward =  -17.0\n"
     ]
    }
   ],
   "source": [
    "get_new_card_action = np.array(0, dtype=np.int32)\n",
    "end_round_action = np.array(1, dtype=np.int32)\n",
    "\n",
    "environment = CardGameEnv()\n",
    "time_step = environment.reset()\n",
    "print(time_step)\n",
    "cumulative_reward = time_step.reward\n",
    "\n",
    "for _ in range(1):\n",
    "  time_step = environment.step(get_new_card_action)\n",
    "  print(time_step)\n",
    "  cumulative_reward += time_step.reward\n",
    "\n",
    "time_step = environment.step(end_round_action)\n",
    "print(time_step)\n",
    "cumulative_reward += time_step.reward\n",
    "print('Final Reward = ', cumulative_reward)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "card_game_env.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "authorship_tag": "ABX9TyO0WRwBcg80OLisWET/RExB",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
